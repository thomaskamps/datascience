{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assignment2\n",
    "\n",
    "## Notebook made by  \n",
    "\n",
    "|** Name** | **Student id** | **email**|\n",
    "|:- |:-|:-|\n",
    "|. | | |\n",
    "|  | |. |\n",
    "\n",
    "### Pledge (taken from [Coursera's Honor Code](https://www.coursera.org/about/terms/honorcode) )\n",
    "\n",
    "\n",
    "\n",
    "Put here a selfie with your photo where you hold a signed paper with the following text: (if this is team work, put two selfies here). The link must be to some place on the web, not to a local file. \n",
    "\n",
    "> My answers to homework, quizzes and exams will be my own work (except for assignments that explicitly permit collaboration).\n",
    "\n",
    ">I will not make solutions to homework, quizzes or exams available to anyone else. This includes both solutions written by me, as well as any official solutions provided by the course staff.\n",
    "\n",
    ">I will not engage in any other activities that will dishonestly improve my results or dishonestly improve/hurt the results of others.\n",
    "\n",
    "<img src='link to your selfie'/>\n",
    "\n",
    "### Note\n",
    "* **Assignments without the selfies or completely filled in information will not be graded and receive 0 points.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part 1: obtaining information from the web\n",
    "\n",
    "### RSS parsing\n",
    "\n",
    "Make a notebook that performs the following steps.\n",
    "\n",
    "1. Create a script that retrieves all urls of rss feeds from <http://www.volkskrant.nl/rss/feeds/>. Use urllib2 and beautifulsoup for this. Store the urls in a list.\n",
    "    * **update 2016**\n",
    "    * As all Dutch sites, Volkskrant asks whether you accept cookies. This makes simple collecting webpages a lot harder. \n",
    "    * The code in the code cell below does the trick. \n",
    "    * After running this, I could collect further files from Volkskrant without additional cookie hassle.\n",
    "2. Download all rss feeds and store them on disk.\n",
    "3. Parse all RSS feeds using `lxml`. Create a list of  dicts with fields \"channel\", \"url\", \"title\", \"date\" in which you store this information for each item.\n",
    "4. Compute some statistics about this dict: how many items, how many per channel, are there doubles (items occuring in several channels), etc.\n",
    "5. Write this list as a csv file, store on disk, and upload to Google fusion tables.\n",
    "6. Download all articles (once), parse out the text and store as pairs (date,text) in a list.\n",
    "7. Count per day the number of words, and the number of unique words. Show this in a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33,\n",
       " [u'http://www.volkskrant.nl/nieuws/rss.xml',\n",
       "  u'http://www.volkskrant.nl/nieuws-voorpagina/rss.xml',\n",
       "  u'http://www.volkskrant.nl/buitenland/rss.xml',\n",
       "  u'http://www.volkskrant.nl/binnenland/rss.xml'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cookielib # Thanks to http://stackoverflow.com/questions/29395407/enabling-cookies-with-urllib\n",
    "import urllib2\n",
    "import urllib\n",
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from lxml import etree\n",
    "from collections import Counter\n",
    "import pprint\n",
    "\n",
    "# Create a script that retrieves all urls of rss feeds from http://www.volkskrant.nl/rss/feeds/. \n",
    "# Use urllib2 and beautifulsoup for this. Store the urls in a list.\n",
    "url = 'http://www.volkskrant.nl/rss/feeds/'\n",
    "\n",
    "# with urllib2 and handling cookies\n",
    "cookiejar= cookielib.LWPCookieJar()\n",
    "opener= urllib2.build_opener( urllib2.HTTPCookieProcessor(cookiejar) )\n",
    "response=opener.open(url)\n",
    "html_doc= ' '.join(response.readlines())\n",
    " \n",
    "rsssoup = BeautifulSoup(html_doc, \"html5lib\")\n",
    "\n",
    "# test \n",
    "list_items = [ref.get('href') for ref in rsssoup.findAll('a') if ref.get('href') and \"rss.xml\" in ref.get('href')]\n",
    "len(list_items), list_items[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oops.. http://www.volkskrant.nl/kijkverder/rss.xml not found (404)\n"
     ]
    }
   ],
   "source": [
    "# Download all rss feeds and store them on disk.\n",
    "\n",
    "#make dir for our files\n",
    "if not os.path.exists(\"VK-data\"):\n",
    "    os.makedirs(\"VK-data\")\n",
    "\n",
    "for rssurl in list_items:\n",
    "    try:\n",
    "        response = urllib2.urlopen(rssurl)\n",
    "        #print \"Downloaded \" + rssurl\n",
    "        xml = response.read()\n",
    "        name = rssurl.replace(\"http://www.volkskrant.nl/\", \"\").replace(\"/rss.xml\", \"\")\n",
    "        with open(\"VK-data/\"+name+'.xml', 'w') as f:\n",
    "            f.write(xml)\n",
    "    except urllib2.HTTPError:\n",
    "        print \"Oops.. \" + rssurl + \" not found (404)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'url': 'http://www.volkskrant.nl/wetenschap/belgische-specialisten-willen-nieuw-betalingssysteem~a4459280/', 'date': 'Wed, 08 Feb 2017 01:00:00 GMT', 'channel': 'VK:Archief', 'title': 'Belgische specialisten willen nieuw betalingssysteem'}, {'url': 'http://www.volkskrant.nl/opinie/-oorlogstaal-aanhangers-coalitiepartijen-baart-zorgen~a4459184/', 'date': 'Wed, 08 Feb 2017 01:00:00 GMT', 'channel': 'VK:Archief', 'title': \"'Oorlogstaal' aanhangers coalitiepartijen baart zorgen\"}]\n"
     ]
    }
   ],
   "source": [
    "# Parse all RSS feeds using lxml. Create a list of dicts with fields \"channel\", \"url\", \"title\", \"date\"\n",
    "# in which you store this information for each item.\n",
    "\n",
    "list_of_items = []\n",
    "for f in os.listdir('VK-data'):\n",
    "    if f.endswith(\".xml\"): \n",
    "        doc = etree.parse('VK-data/'+f)\n",
    "        root = doc.getroot().find('channel')\n",
    "        channel = root.findtext('title')\n",
    "        items = [{\"title\": item.findtext('title'), \"url\": item.findtext(\"link\"),\n",
    "                 \"date\": item.findtext('pubDate'), \"channel\": channel}\n",
    "                 for item in root.getiterator('item')]\n",
    "        list_of_items += items\n",
    "\n",
    "print list_of_items[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The list of dicts contains 1174 items.\n",
      "This dict shows how many items are in each channel: \n",
      "{'VK:Archief': 40,\n",
      " 'VK:Beeldende Kunst': 40,\n",
      " 'VK:Binnenland': 40,\n",
      " 'VK:Boeken': 40,\n",
      " 'VK:Buitenland': 40,\n",
      " 'VK:Cartoons': 14,\n",
      " 'VK:Economie': 40,\n",
      " 'VK:Festivals': 40,\n",
      " 'VK:Film': 40,\n",
      " 'VK:Foto': 40,\n",
      " 'VK:Koken & Eten': 40,\n",
      " 'VK:Magazine': 40,\n",
      " 'VK:Media': 40,\n",
      " 'VK:Mode & Mooi': 40,\n",
      " 'VK:Muziek': 40,\n",
      " 'VK:Opinie': 40,\n",
      " 'VK:Politiek': 40,\n",
      " 'VK:Recensies': 40,\n",
      " 'VK:Reizen': 40,\n",
      " 'VK:Sport': 40,\n",
      " 'VK:Tech': 40,\n",
      " 'VK:Televisie': 40,\n",
      " 'VK:Theater': 40,\n",
      " 'VK:Vonk': 40,\n",
      " 'VK:Voordeel': 40,\n",
      " 'VK:Voorpagina': 160,\n",
      " 'VK:Wetenschap': 40}\n",
      "There are 202 doubles.\n"
     ]
    }
   ],
   "source": [
    "# Compute some statistics about this dict: how many items, how many per channel, \n",
    "# are there doubles (items occuring in several channels), etc.\n",
    "\n",
    "print \"The list of dicts contains \" + str(len(list_of_items)) + \" items.\"\n",
    "\n",
    "count_per_channel = Counter([item['channel'] for item in list_of_items])\n",
    "print \"This dict shows how many items are in each channel: \"\n",
    "pprint.pprint(dict(count_per_channel))\n",
    "\n",
    "# Use URL's to identify unique items\n",
    "count_url = Counter([item['url'] for item in list_of_items])\n",
    "if len(count_url) == len(list_of_items):\n",
    "    print \"No doubles\"\n",
    "else:\n",
    "    print \"There are \" + str(len([count for count in count_url if count_url[count] > 1])) + \" doubles.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'http://www.volkskrant.nl/archief/de-teloorgang-van-de-school-voor-bijzonder-onderwijs~a4456060/'\n",
    "def download_article(url):\n",
    "    try:\n",
    "        response = urllib2.urlopen(url)\n",
    "        #print \"Downloaded \" + rssurl\n",
    "        data = response.read()\n",
    "        print data\n",
    "    except urllib2.HTTPError:\n",
    "        print \"Oops.. \" + rssurl + \" not found (404)\"\n",
    "download_article(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### JSON parsing\n",
    "\n",
    "1. Download <http://maartenmarx.nl/teaching/ISatWork/NoteBooks/consuming-json-data-from-a-web-service.ipynb>, remove all code blocks, and turn it into a notebook again. \n",
    "2. Check that what you did is correct and you did not remove too much using a notebook viewer.\n",
    "3. Now extract all code from the downloaded notebook, save it to a file, and execute it as a Python script. Does it give errors? Is it syntactically correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PDF parsing\n",
    "\n",
    "1. Save a wordfile as PDF, open it in Python, extract all text. Describe the differences, if any. Try the same with a two column PDF file from the web. This exercise gets more interesting if you use _difficult_ PDF. Why not try <http://wch.github.io/latexsheet/latexsheet.pdf>?\n",
    "\n",
    "* Is the word order still as it should be?\n",
    "* What about the strange characters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part 2 Python recap\n",
    "\n",
    "Download [PythonRecap2.0.ipynb](PythonRecap2.0.ipynb),  and answer all questions as asked."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
